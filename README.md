# IntentFormer
![image](https://github.com/neha013/IntentFormer/assets/41139808/36397937-e07a-4f53-8a64-7d472cd1bad3)

This is the official Python implementation for the paper, *[Predicting Pedestrian Intentions with Multimodal IntentFormer: A Co-Learning Approach](https://www.google.com "Google's Homepage")*. authored by ***Neha Sharma, Chhavi Dhiman, S. Indu***

The prediction of pedestrian crossing intention is a crucial task in the context of autonomous driving for ensuring traffic safety and reducing the risk of accidents without human intervention. Nevertheless, the complexity of pedestrian behaviour, which is influenced by numerous contextual factors in conjunction with visual appearance cues and past trajectory, poses a significant challenge. Several state-of-the-art approaches have recently emerged that incorporate multiple modalities. Nonetheless, the suboptimal modality integration techniques in these approaches fail to capture the intricate intermodal relationships and robustly represent pedestrian-environment interactions in challenging scenarios. To address these issues, a novel Multimodal IntentFormer architecture is presented. It works with three transformer encoders $TE_1,TE_2,TE_3$  which learn RGB, segmentation maps and trajectory paths in a co-learning environment controlled by a Co-learning module. A novel Co-learning Adaptive Composite (CAC) loss function is also proposed, which penalizes different stages of the architecture, regularizing the model and mitigating the risk of overfitting.  Each encoder $TE_i$  applies the concept of the Multi-Head Shared Weight Attention (MHSWA) mechanism while learning three modalities in the proposed co-learning approach. The proposed architecture outperforms existing state-of-the-art approaches on benchmark datasets, PIE and JAAD with 93% and 92% accuracy respectively. Furthermore, extensive ablation studies demonstrate the efficiency and robustness of the architecture, even under varying Time-to-event (TTE) and observation lengths. 
